{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/sonder-art/bandit_simulator/blob/main/markov_bandit_sim.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from typing import Tuple, List, Dict, Callable, Optional, Union\n",
    "import time\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(None)  # Use system entropy for true randomness\n",
    "random.seed(None)  # Use system entropy for true randomness\n",
    "\n",
    "# ================ MARKOV BANDIT ENVIRONMENT CLASSES ================\n",
    "\n",
    "class MarkovBanditEnvironment:\n",
    "    \"\"\"Base class for two-armed Markov bandit environments.\"\"\"\n",
    "    \n",
    "    def __init__(self, T: int = 100, exploration_bonus: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initialize the Markov bandit environment.\n",
    "        \n",
    "        Args:\n",
    "            T (int): Number of turns in the game\n",
    "            exploration_bonus (float): Coefficient for exploration bonus\n",
    "        \"\"\"\n",
    "        self.T = T\n",
    "        self.current_turn = 0\n",
    "        self.exploration_bonus = exploration_bonus\n",
    "        self.states = None  # Hidden states (X_t^1, X_t^2)\n",
    "        self.history = {\n",
    "            'actions': [],\n",
    "            'rewards': [],\n",
    "            'states': [],\n",
    "            'exploration_bonus': []\n",
    "        }\n",
    "    \n",
    "    def reset(self, T: Optional[int] = None) -> None:\n",
    "        \"\"\"\n",
    "        Reset the environment for a new game.\n",
    "        \n",
    "        Args:\n",
    "            T (int, optional): Number of turns for the new game\n",
    "        \"\"\"\n",
    "        if T is not None:\n",
    "            self.T = T\n",
    "        self.current_turn = 0\n",
    "        self.history = {\n",
    "            'actions': [],\n",
    "            'rewards': [],\n",
    "            'states': [],\n",
    "            'exploration_bonus': []\n",
    "        }\n",
    "        self._initialize_states()\n",
    "    \n",
    "    def _initialize_states(self) -> None:\n",
    "        \"\"\"Initialize the hidden states for both arms.\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "    \n",
    "    def _update_states(self, action: int) -> None:\n",
    "        \"\"\"\n",
    "        Update the hidden states based on the current states and action.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "    \n",
    "    def _compute_reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute the reward based on the current state of the chosen arm.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \n",
    "        Returns:\n",
    "            float: The reward obtained\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
    "    \n",
    "    def _compute_exploration_bonus(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute the exploration bonus for the chosen action.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \n",
    "        Returns:\n",
    "            float: The exploration bonus\n",
    "        \"\"\"\n",
    "        # Default implementation (to be overridden by subclasses if needed)\n",
    "        # Simple count-based bonus that decreases with the number of times the arm has been pulled\n",
    "        action_counts = [self.history['actions'].count(0), self.history['actions'].count(1)]\n",
    "        if action_counts[action] == 0:\n",
    "            return self.exploration_bonus\n",
    "        return self.exploration_bonus / np.sqrt(action_counts[action])\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Take a step in the environment by selecting an arm.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[float, float]: The reward and exploration bonus obtained\n",
    "        \"\"\"\n",
    "        if self.current_turn >= self.T:\n",
    "            raise ValueError(\"Game is over, please reset.\")\n",
    "        \n",
    "        # Record current state\n",
    "        self.history['states'].append(self.states.copy())\n",
    "        \n",
    "        # Determine reward based on action and current state\n",
    "        reward = self._compute_reward(action)\n",
    "        \n",
    "        # Compute exploration bonus\n",
    "        exploration_bonus = self._compute_exploration_bonus(action)\n",
    "        \n",
    "        # Update history\n",
    "        self.history['actions'].append(action)\n",
    "        self.history['rewards'].append(reward)\n",
    "        self.history['exploration_bonus'].append(exploration_bonus)\n",
    "        \n",
    "        # Update states based on action\n",
    "        self._update_states(action)\n",
    "        \n",
    "        # Increment turn\n",
    "        self.current_turn += 1\n",
    "        \n",
    "        return reward, exploration_bonus\n",
    "    \n",
    "    def get_visible_info(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get the visible information for the agent.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Dictionary with visible information\n",
    "        \"\"\"\n",
    "        # By default, agents only see rewards, actions, turn info, and exploration bonus\n",
    "        return {\n",
    "            'current_turn': self.current_turn,\n",
    "            'total_turns': self.T,\n",
    "            'history': {\n",
    "                'actions': self.history['actions'],\n",
    "                'rewards': self.history['rewards'],\n",
    "                'exploration_bonus': self.history['exploration_bonus']\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class Problem1Environment(MarkovBanditEnvironment):\n",
    "    \"\"\"\n",
    "    Problem 1: Independent, Known Transitions, Fixed T, Hidden State + Exploration Bonus\n",
    "    \n",
    "    Each arm follows an independent Markov process with known transition matrices.\n",
    "    The states are hidden from the agent, but the transition matrices are known.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, T: int = 100, exploration_bonus: float = 0.1, \n",
    "                 state_count: int = 2, state_rewards: Optional[List[float]] = None):\n",
    "        \"\"\"\n",
    "        Initialize Problem 1 environment.\n",
    "        \n",
    "        Args:\n",
    "            T (int): Number of turns\n",
    "            exploration_bonus (float): Coefficient for exploration bonus\n",
    "            state_count (int): Number of possible states for each arm\n",
    "            state_rewards (List[float], optional): Rewards for each state\n",
    "        \"\"\"\n",
    "        super().__init__(T, exploration_bonus)\n",
    "        self.state_count = state_count\n",
    "        \n",
    "        # If state rewards are not provided, use default values [0.2, 0.8]\n",
    "        self.state_rewards = state_rewards if state_rewards is not None else [0.2, 0.8]\n",
    "        \n",
    "        # Generate transition matrices for both arms\n",
    "        self.transition_matrices = [\n",
    "            self._generate_transition_matrix(state_count) for _ in range(2)\n",
    "        ]\n",
    "    \n",
    "    def _generate_transition_matrix(self, state_count: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate a random transition matrix for a Markov process.\n",
    "        \n",
    "        Args:\n",
    "            state_count (int): Number of states\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Transition matrix of shape (state_count, state_count)\n",
    "        \"\"\"\n",
    "        # Generate a random transition matrix where each row sums to 1\n",
    "        matrix = np.random.dirichlet(np.ones(state_count), size=state_count)\n",
    "        return matrix\n",
    "    \n",
    "    def _initialize_states(self) -> None:\n",
    "        \"\"\"Initialize the hidden states for both arms.\"\"\"\n",
    "        # Randomly initialize the state for each arm\n",
    "        self.states = [\n",
    "            np.random.randint(0, self.state_count) for _ in range(2)\n",
    "        ]\n",
    "    \n",
    "    def _update_states(self, action: int) -> None:\n",
    "        \"\"\"\n",
    "        Update the hidden states based on the current states and the known transition matrices.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \"\"\"\n",
    "        # Update each arm's state independently according to its transition matrix\n",
    "        for arm in range(2):\n",
    "            current_state = self.states[arm]\n",
    "            transition_probabilities = self.transition_matrices[arm][current_state]\n",
    "            self.states[arm] = np.random.choice(self.state_count, p=transition_probabilities)\n",
    "    \n",
    "    def _compute_reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute the reward based on the current state of the chosen arm.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \n",
    "        Returns:\n",
    "            float: The reward obtained\n",
    "        \"\"\"\n",
    "        # Get the current state of the chosen arm\n",
    "        current_state = self.states[action]\n",
    "        \n",
    "        # The reward is Bernoulli with probability depending on the state\n",
    "        reward_probability = self.state_rewards[current_state]\n",
    "        return 1.0 if np.random.random() < reward_probability else 0.0\n",
    "    \n",
    "    def get_visible_info(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get the visible information for the agent.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Dictionary with visible information including known transition matrices\n",
    "        \"\"\"\n",
    "        info = super().get_visible_info()\n",
    "        info['transition_matrices'] = self.transition_matrices\n",
    "        info['state_rewards'] = self.state_rewards\n",
    "        return info\n",
    "\n",
    "\n",
    "class Problem2Environment(MarkovBanditEnvironment):\n",
    "    \"\"\"\n",
    "    Problem 2: Independent, Unknown Transitions, Fixed T, Hidden State + Exploration Bonus\n",
    "    \n",
    "    Each arm follows an independent Markov process with unknown transition matrices.\n",
    "    The states are hidden from the agent, and the transition matrices are unknown.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, T: int = 100, exploration_bonus: float = 0.1, \n",
    "                 state_count: int = 2, state_rewards: Optional[List[float]] = None):\n",
    "        \"\"\"\n",
    "        Initialize Problem 2 environment.\n",
    "        \n",
    "        Args:\n",
    "            T (int): Number of turns\n",
    "            exploration_bonus (float): Coefficient for exploration bonus\n",
    "            state_count (int): Number of possible states for each arm\n",
    "            state_rewards (List[float], optional): Rewards for each state\n",
    "        \"\"\"\n",
    "        super().__init__(T, exploration_bonus)\n",
    "        self.state_count = state_count\n",
    "        \n",
    "        # If state rewards are not provided, use default values [0.2, 0.8]\n",
    "        self.state_rewards = state_rewards if state_rewards is not None else [0.2, 0.8]\n",
    "        \n",
    "        # Generate transition matrices for both arms (unknown to the agent)\n",
    "        self.transition_matrices = [\n",
    "            self._generate_transition_matrix(state_count) for _ in range(2)\n",
    "        ]\n",
    "    \n",
    "    def _generate_transition_matrix(self, state_count: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate a random transition matrix for a Markov process.\n",
    "        \n",
    "        Args:\n",
    "            state_count (int): Number of states\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Transition matrix of shape (state_count, state_count)\n",
    "        \"\"\"\n",
    "        # Generate a random transition matrix where each row sums to 1\n",
    "        matrix = np.random.dirichlet(np.ones(state_count), size=state_count)\n",
    "        return matrix\n",
    "    \n",
    "    def _initialize_states(self) -> None:\n",
    "        \"\"\"Initialize the hidden states for both arms.\"\"\"\n",
    "        # Randomly initialize the state for each arm\n",
    "        self.states = [\n",
    "            np.random.randint(0, self.state_count) for _ in range(2)\n",
    "        ]\n",
    "    \n",
    "    def _update_states(self, action: int) -> None:\n",
    "        \"\"\"\n",
    "        Update the hidden states based on the current states and the transition matrices.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \"\"\"\n",
    "        # Update each arm's state independently according to its transition matrix\n",
    "        for arm in range(2):\n",
    "            current_state = self.states[arm]\n",
    "            transition_probabilities = self.transition_matrices[arm][current_state]\n",
    "            self.states[arm] = np.random.choice(self.state_count, p=transition_probabilities)\n",
    "    \n",
    "    def _compute_reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute the reward based on the current state of the chosen arm.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \n",
    "        Returns:\n",
    "            float: The reward obtained\n",
    "        \"\"\"\n",
    "        # Get the current state of the chosen arm\n",
    "        current_state = self.states[action]\n",
    "        \n",
    "        # The reward is Bernoulli with probability depending on the state\n",
    "        reward_probability = self.state_rewards[current_state]\n",
    "        return 1.0 if np.random.random() < reward_probability else 0.0\n",
    "    \n",
    "    def get_visible_info(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get the visible information for the agent.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Dictionary with visible information (transitions are unknown to the agent)\n",
    "        \"\"\"\n",
    "        # Same as base class, no transition information is provided\n",
    "        return super().get_visible_info()\n",
    "\n",
    "\n",
    "class Problem3Environment(MarkovBanditEnvironment):\n",
    "    \"\"\"\n",
    "    Problem 3: Dependent (Joint), Known Transitions, Fixed T, Hidden State + Exploration Bonus\n",
    "    \n",
    "    Both arms follow a joint Markov process with known transition matrix.\n",
    "    The states are hidden from the agent, but the joint transition matrix is known.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, T: int = 100, exploration_bonus: float = 0.1, \n",
    "                 state_count: int = 2, state_rewards: Optional[List[float]] = None):\n",
    "        \"\"\"\n",
    "        Initialize Problem 3 environment.\n",
    "        \n",
    "        Args:\n",
    "            T (int): Number of turns\n",
    "            exploration_bonus (float): Coefficient for exploration bonus\n",
    "            state_count (int): Number of possible states for each arm\n",
    "            state_rewards (List[float], optional): Rewards for each state\n",
    "        \"\"\"\n",
    "        super().__init__(T, exploration_bonus)\n",
    "        self.state_count = state_count\n",
    "        \n",
    "        # If state rewards are not provided, use default values [0.2, 0.8]\n",
    "        self.state_rewards = state_rewards if state_rewards is not None else [0.2, 0.8]\n",
    "        \n",
    "        # Total number of joint states (state_count^2)\n",
    "        self.joint_state_count = state_count ** 2\n",
    "        \n",
    "        # Generate joint transition matrix\n",
    "        # This is a matrix of shape (joint_state_count, joint_state_count) where\n",
    "        # joint_state = state1 * state_count + state2\n",
    "        self.joint_transition_matrix = self._generate_joint_transition_matrix()\n",
    "    \n",
    "    def _generate_joint_transition_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate a random joint transition matrix for both arms.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Joint transition matrix of shape (joint_state_count, joint_state_count)\n",
    "        \"\"\"\n",
    "        # Generate a random transition matrix where each row sums to 1\n",
    "        matrix = np.random.dirichlet(np.ones(self.joint_state_count), size=self.joint_state_count)\n",
    "        return matrix\n",
    "    \n",
    "    def _initialize_states(self) -> None:\n",
    "        \"\"\"Initialize the hidden states for both arms.\"\"\"\n",
    "        # Randomly initialize the state for each arm\n",
    "        self.states = [\n",
    "            np.random.randint(0, self.state_count) for _ in range(2)\n",
    "        ]\n",
    "    \n",
    "    def _joint_state_to_individual(self, joint_state: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Convert a joint state index to individual arm states.\n",
    "        \n",
    "        Args:\n",
    "            joint_state (int): Joint state index\n",
    "        \n",
    "        Returns:\n",
    "            List[int]: Individual states [state1, state2]\n",
    "        \"\"\"\n",
    "        state1 = joint_state // self.state_count\n",
    "        state2 = joint_state % self.state_count\n",
    "        return [state1, state2]\n",
    "    \n",
    "    def _individual_to_joint_state(self, states: List[int]) -> int:\n",
    "        \"\"\"\n",
    "        Convert individual arm states to a joint state index.\n",
    "        \n",
    "        Args:\n",
    "            states (List[int]): Individual states [state1, state2]\n",
    "        \n",
    "        Returns:\n",
    "            int: Joint state index\n",
    "        \"\"\"\n",
    "        return states[0] * self.state_count + states[1]\n",
    "    \n",
    "    def _update_states(self, action: int) -> None:\n",
    "        \"\"\"\n",
    "        Update the hidden states based on the current joint state and the joint transition matrix.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \"\"\"\n",
    "        # Convert individual states to joint state\n",
    "        current_joint_state = self._individual_to_joint_state(self.states)\n",
    "        \n",
    "        # Get transition probabilities for the current joint state\n",
    "        transition_probabilities = self.joint_transition_matrix[current_joint_state]\n",
    "        \n",
    "        # Sample next joint state\n",
    "        next_joint_state = np.random.choice(self.joint_state_count, p=transition_probabilities)\n",
    "        \n",
    "        # Convert back to individual states\n",
    "        self.states = self._joint_state_to_individual(next_joint_state)\n",
    "    \n",
    "    def _compute_reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute the reward based on the current state of the chosen arm.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \n",
    "        Returns:\n",
    "            float: The reward obtained\n",
    "        \"\"\"\n",
    "        # Get the current state of the chosen arm\n",
    "        current_state = self.states[action]\n",
    "        \n",
    "        # The reward is Bernoulli with probability depending on the state\n",
    "        reward_probability = self.state_rewards[current_state]\n",
    "        return 1.0 if np.random.random() < reward_probability else 0.0\n",
    "    \n",
    "    def get_visible_info(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get the visible information for the agent.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Dictionary with visible information including known joint transition matrix\n",
    "        \"\"\"\n",
    "        info = super().get_visible_info()\n",
    "        info['joint_transition_matrix'] = self.joint_transition_matrix\n",
    "        info['state_count'] = self.state_count\n",
    "        info['state_rewards'] = self.state_rewards\n",
    "        return info\n",
    "\n",
    "\n",
    "class Problem4Environment(MarkovBanditEnvironment):\n",
    "    \"\"\"\n",
    "    Problem 4: Partially Observable / Possibly Unknown / Possibly Random T\n",
    "    \n",
    "    This is the most general case: states are hidden, transitions can be known or unknown,\n",
    "    and the horizon can be random or infinite.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, T: int = 100, exploration_bonus: float = 0.1, \n",
    "                 state_count: int = 2, state_rewards: Optional[List[float]] = None,\n",
    "                 known_transitions: bool = False, random_horizon: bool = False,\n",
    "                 joint_dynamics: bool = False, discount_factor: float = 0.95):\n",
    "        \"\"\"\n",
    "        Initialize Problem 4 environment.\n",
    "        \n",
    "        Args:\n",
    "            T (int): Number of turns or expected turns if random_horizon is True\n",
    "            exploration_bonus (float): Coefficient for exploration bonus\n",
    "            state_count (int): Number of possible states for each arm\n",
    "            state_rewards (List[float], optional): Rewards for each state\n",
    "            known_transitions (bool): Whether transitions are known to the agent\n",
    "            random_horizon (bool): Whether the time horizon is random\n",
    "            joint_dynamics (bool): Whether arms have joint dynamics or independent dynamics\n",
    "            discount_factor (float): Discount factor for infinite horizon (if random_horizon is True)\n",
    "        \"\"\"\n",
    "        super().__init__(T, exploration_bonus)\n",
    "        self.state_count = state_count\n",
    "        \n",
    "        # If state rewards are not provided, use default values [0.2, 0.8]\n",
    "        self.state_rewards = state_rewards if state_rewards is not None else [0.2, 0.8]\n",
    "        \n",
    "        self.known_transitions = known_transitions\n",
    "        self.random_horizon = random_horizon\n",
    "        self.joint_dynamics = joint_dynamics\n",
    "        self.discount_factor = discount_factor\n",
    "        \n",
    "        # Generate transition matrices\n",
    "        if joint_dynamics:\n",
    "            self.joint_state_count = state_count ** 2\n",
    "            self.joint_transition_matrix = self._generate_joint_transition_matrix()\n",
    "            self.transition_matrices = None\n",
    "        else:\n",
    "            self.transition_matrices = [\n",
    "                self._generate_transition_matrix(state_count) for _ in range(2)\n",
    "            ]\n",
    "            self.joint_transition_matrix = None\n",
    "        \n",
    "        # If using a random horizon, generate the actual horizon\n",
    "        if random_horizon:\n",
    "            # Generate a geometric random variable with mean T\n",
    "            p = 1.0 / T\n",
    "            self.actual_horizon = np.random.geometric(p)\n",
    "        else:\n",
    "            self.actual_horizon = T\n",
    "    \n",
    "    def _generate_transition_matrix(self, state_count: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate a random transition matrix for a Markov process.\n",
    "        \n",
    "        Args:\n",
    "            state_count (int): Number of states\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Transition matrix of shape (state_count, state_count)\n",
    "        \"\"\"\n",
    "        matrix = np.random.dirichlet(np.ones(state_count), size=state_count)\n",
    "        return matrix\n",
    "    \n",
    "    def _generate_joint_transition_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate a random joint transition matrix for both arms.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Joint transition matrix of shape (joint_state_count, joint_state_count)\n",
    "        \"\"\"\n",
    "        matrix = np.random.dirichlet(np.ones(self.joint_state_count), size=self.joint_state_count)\n",
    "        return matrix\n",
    "    \n",
    "    def _initialize_states(self) -> None:\n",
    "        \"\"\"Initialize the hidden states for both arms.\"\"\"\n",
    "        # Randomly initialize the state for each arm\n",
    "        self.states = [\n",
    "            np.random.randint(0, self.state_count) for _ in range(2)\n",
    "        ]\n",
    "    \n",
    "    def _joint_state_to_individual(self, joint_state: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Convert a joint state index to individual arm states.\n",
    "        \n",
    "        Args:\n",
    "            joint_state (int): Joint state index\n",
    "        \n",
    "        Returns:\n",
    "            List[int]: Individual states [state1, state2]\n",
    "        \"\"\"\n",
    "        state1 = joint_state // self.state_count\n",
    "        state2 = joint_state % self.state_count\n",
    "        return [state1, state2]\n",
    "    \n",
    "    def _individual_to_joint_state(self, states: List[int]) -> int:\n",
    "        \"\"\"\n",
    "        Convert individual arm states to a joint state index.\n",
    "        \n",
    "        Args:\n",
    "            states (List[int]): Individual states [state1, state2]\n",
    "        \n",
    "        Returns:\n",
    "            int: Joint state index\n",
    "        \"\"\"\n",
    "        return states[0] * self.state_count + states[1]\n",
    "    \n",
    "    def _update_states(self, action: int) -> None:\n",
    "        \"\"\"\n",
    "        Update the hidden states based on the current states and the transition matrices.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \"\"\"\n",
    "        if self.joint_dynamics:\n",
    "            # Convert individual states to joint state\n",
    "            current_joint_state = self._individual_to_joint_state(self.states)\n",
    "            \n",
    "            # Get transition probabilities for the current joint state\n",
    "            transition_probabilities = self.joint_transition_matrix[current_joint_state]\n",
    "            \n",
    "            # Sample next joint state\n",
    "            next_joint_state = np.random.choice(self.joint_state_count, p=transition_probabilities)\n",
    "            \n",
    "            # Convert back to individual states\n",
    "            self.states = self._joint_state_to_individual(next_joint_state)\n",
    "        else:\n",
    "            # Update each arm's state independently\n",
    "            for arm in range(2):\n",
    "                current_state = self.states[arm]\n",
    "                transition_probabilities = self.transition_matrices[arm][current_state]\n",
    "                self.states[arm] = np.random.choice(self.state_count, p=transition_probabilities)\n",
    "    \n",
    "    def _compute_reward(self, action: int) -> float:\n",
    "        \"\"\"\n",
    "        Compute the reward based on the current state of the chosen arm.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \n",
    "        Returns:\n",
    "            float: The reward obtained\n",
    "        \"\"\"\n",
    "        # Get the current state of the chosen arm\n",
    "        current_state = self.states[action]\n",
    "        \n",
    "        # The reward is Bernoulli with probability depending on the state\n",
    "        reward_probability = self.state_rewards[current_state]\n",
    "        return 1.0 if np.random.random() < reward_probability else 0.0\n",
    "    \n",
    "    def get_visible_info(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Get the visible information for the agent.\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Dictionary with visible information\n",
    "        \"\"\"\n",
    "        info = super().get_visible_info()\n",
    "        \n",
    "        # Add transition information if known\n",
    "        if self.known_transitions:\n",
    "            if self.joint_dynamics:\n",
    "                info['joint_transition_matrix'] = self.joint_transition_matrix\n",
    "                info['joint_dynamics'] = True\n",
    "            else:\n",
    "                info['transition_matrices'] = self.transition_matrices\n",
    "                info['joint_dynamics'] = False\n",
    "            \n",
    "            info['state_rewards'] = self.state_rewards\n",
    "        \n",
    "        # If horizon is random, don't provide the actual horizon\n",
    "        if self.random_horizon:\n",
    "            info['random_horizon'] = True\n",
    "            info['discount_factor'] = self.discount_factor\n",
    "            # Remove total_turns since it's unknown\n",
    "            info.pop('total_turns', None)\n",
    "        \n",
    "        return info\n",
    "    \n",
    "    def step(self, action: int) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Take a step in the environment by selecting an arm.\n",
    "        \n",
    "        Args:\n",
    "            action (int): The arm chosen (0 for arm 1, 1 for arm 2)\n",
    "        \n",
    "        Returns:\n",
    "            Tuple[float, float]: The reward and exploration bonus obtained\n",
    "        \"\"\"\n",
    "        # Check if the episode has ended (for random horizon)\n",
    "        if self.random_horizon:\n",
    "            if self.current_turn >= self.actual_horizon:\n",
    "                raise ValueError(\"Game is over, please reset.\")\n",
    "        else:\n",
    "            if self.current_turn >= self.T:\n",
    "                raise ValueError(\"Game is over, please reset.\")\n",
    "        \n",
    "        # Use the parent's step method\n",
    "        return super().step(action)\n",
    "\n",
    "\n",
    "# ================ EXPERIMENT RUNNER ================\n",
    "\n",
    "def run_experiment(\n",
    "    agent_func: Callable,\n",
    "    env_class: Callable,\n",
    "    env_params: Dict = None,\n",
    "    n_games: int = 50,\n",
    "    default_turns: int = 100,\n",
    "    random_turns: bool = False,\n",
    "    verbose: bool = True,\n",
    "    pbar: Optional[tqdm] = None\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run an experiment with the given agent on a specific Markov bandit environment.\n",
    "    \n",
    "    Args:\n",
    "        agent_func (Callable): The agent function to use\n",
    "        env_class (Callable): The environment class to use\n",
    "        env_params (Dict, optional): Additional parameters for the environment\n",
    "        n_games (int): Number of games to play per environment\n",
    "        default_turns (int): Default number of turns per game (used when random_turns=False)\n",
    "        random_turns (bool): Whether to use random number of turns\n",
    "        verbose (bool): Whether to show progress bar\n",
    "        pbar (tqdm, optional): External progress bar to update instead of creating a new one\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results of the experiment\n",
    "    \"\"\"\n",
    "    # Force different random seed each time\n",
    "    current_time = time.time()\n",
    "    np.random.seed(int(current_time * 1000) % 10000)\n",
    "    random.seed(int(current_time * 2000) % 10000)\n",
    "    \n",
    "    # Default environment parameters\n",
    "    if env_params is None:\n",
    "        env_params = {}\n",
    "    \n",
    "    # Create environment\n",
    "    env = env_class(**env_params)\n",
    "    env_name = env_class.__name__\n",
    "    \n",
    "    results = {\n",
    "        \"environment\": [],\n",
    "        \"game\": [],\n",
    "        \"total_reward\": [],\n",
    "        \"average_reward\": [],\n",
    "        \"turns\": [],\n",
    "        \"actions\": [],\n",
    "        \"optimal_actions\": [],\n",
    "        \"regret\": [],\n",
    "        \"exploration_bonus\": []\n",
    "    }\n",
    "    \n",
    "    # Create turn counts in advance for reproducibility\n",
    "    all_turn_counts = []\n",
    "    if random_turns:\n",
    "        for _ in range(n_games):\n",
    "            all_turn_counts.append(np.random.randint(1, 301))\n",
    "    else:\n",
    "        all_turn_counts = [default_turns] * n_games\n",
    "    \n",
    "    # Use tqdm for progress if verbose and no external pbar\n",
    "    if verbose and pbar is None:\n",
    "        game_iterator = tqdm(range(n_games), desc=f\"Running {env_name}\")\n",
    "    elif pbar is not None:\n",
    "        # Use external progress bar without creating a range iterator\n",
    "        game_iterator = range(n_games)\n",
    "    else:\n",
    "        game_iterator = range(n_games)\n",
    "    \n",
    "    for game_idx in game_iterator:\n",
    "        # Get pre-generated turn count for this game\n",
    "        T = all_turn_counts[game_idx]\n",
    "        \n",
    "        env.reset(T)\n",
    "        \n",
    "        total_reward = 0\n",
    "        total_exploration_bonus = 0\n",
    "        optimal_actions = 0\n",
    "        \n",
    "        # Start the game\n",
    "        game_over = False\n",
    "        while not game_over:\n",
    "            try:\n",
    "                # Get visible information for the agent\n",
    "                visible_info = env.get_visible_info()\n",
    "                \n",
    "                # Get action from agent\n",
    "                action = agent_func(visible_info)\n",
    "                \n",
    "                # Take step in environment\n",
    "                reward, exploration_bonus = env.step(action)\n",
    "                total_reward += reward\n",
    "                total_exploration_bonus += exploration_bonus\n",
    "                \n",
    "                # Count optimal actions (not easily determined in hidden state environments)\n",
    "                # For simplicity, we'll consider an action optimal if it gave a reward\n",
    "                if reward > 0:\n",
    "                    optimal_actions += 1\n",
    "                \n",
    "            except ValueError as e:\n",
    "                # Game is over\n",
    "                game_over = True\n",
    "        \n",
    "        # For Problem 4 with random horizon, record the actual horizon used\n",
    "        if hasattr(env, 'actual_horizon'):\n",
    "            turns_played = env.actual_horizon\n",
    "        else:\n",
    "            turns_played = T\n",
    "        \n",
    "        # Calculate regret (difficult to define perfectly for Markov bandits)\n",
    "        # We'll use a simplified approach: regret = max possible reward - actual reward\n",
    "        # where max possible reward assumes perfect knowledge of states\n",
    "        # This is an approximation and not the true regret\n",
    "        regret = turns_played - total_reward\n",
    "        \n",
    "        # Store results\n",
    "        results[\"environment\"].append(env_name)\n",
    "        results[\"game\"].append(game_idx)\n",
    "        results[\"total_reward\"].append(total_reward)\n",
    "        results[\"average_reward\"].append(total_reward / turns_played)\n",
    "        results[\"turns\"].append(turns_played)\n",
    "        results[\"actions\"].append(env.history['actions'])\n",
    "        results[\"optimal_actions\"].append(optimal_actions / turns_played * 100)  # percentage\n",
    "        results[\"regret\"].append(regret)\n",
    "        results[\"exploration_bonus\"].append(total_exploration_bonus)\n",
    "        \n",
    "        # Update external progress bar if provided\n",
    "        if pbar is not None:\n",
    "            pbar.update(1)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_standard_experiment(agent_func: Callable, problem_number: int, n_experiments: int = 100, \n",
    "                           fixed_turns: bool = True, exploration_bonus: float = 0.1) -> Dict:\n",
    "    \"\"\"\n",
    "    Run a standardized experiment for a specific agent on a specific problem.\n",
    "    \n",
    "    Args:\n",
    "        agent_func (Callable): The agent function to use\n",
    "        problem_number (int): Problem number (1-4)\n",
    "        n_experiments (int): Number of experiments to run (default 100)\n",
    "        fixed_turns (bool): Whether to use fixed T=100 or random T (1-300)\n",
    "        exploration_bonus (float): Coefficient for exploration bonus (β)\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results of the experiment\n",
    "    \"\"\"\n",
    "    # Force different random seed for this experiment\n",
    "    current_time = time.time()\n",
    "    np.random.seed(int(current_time * 1000) % 10000)\n",
    "    random.seed(int(current_time * 2000) % 10000)\n",
    "    \n",
    "    # Set environment class based on problem number\n",
    "    env_classes = {\n",
    "        1: Problem1Environment,\n",
    "        2: Problem2Environment,\n",
    "        3: Problem3Environment,\n",
    "        4: Problem4Environment\n",
    "    }\n",
    "    \n",
    "    if problem_number not in env_classes:\n",
    "        raise ValueError(f\"Invalid problem number: {problem_number}. Must be 1-4.\")\n",
    "    \n",
    "    env_class = env_classes[problem_number]\n",
    "    \n",
    "    # Set environment parameters\n",
    "    env_params = {\n",
    "        'exploration_bonus': exploration_bonus,\n",
    "        'state_count': 2\n",
    "    }\n",
    "    \n",
    "    # For Problem 4, add additional parameters\n",
    "    if problem_number == 4:\n",
    "        env_params.update({\n",
    "            'known_transitions': True,  # Set to False to test with unknown transitions\n",
    "            'random_horizon': not fixed_turns,  # Use random horizon if not fixed_turns\n",
    "            'joint_dynamics': False,  # Set to True to test with joint dynamics\n",
    "            'discount_factor': 0.95  # Used for random/infinite horizon\n",
    "        })\n",
    "    \n",
    "    # Run the experiment\n",
    "    return run_experiment(\n",
    "        agent_func=agent_func,\n",
    "        env_class=env_class,\n",
    "        env_params=env_params,\n",
    "        n_games=n_experiments,\n",
    "        default_turns=100,\n",
    "        random_turns=not fixed_turns,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "\n",
    "# ================ VISUALIZATION FUNCTIONS ================\n",
    "\n",
    "def plot_rewards_by_environment(results: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Plot average rewards for the environment.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'Environment': results['environment'],\n",
    "        'Game': results['game'],\n",
    "        'Average Reward': results['average_reward']\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='Environment', y='Average Reward', data=df)\n",
    "    plt.title('Distribution of Average Rewards')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_optimal_actions(results: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Plot percentage of optimal actions.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'Environment': results['environment'],\n",
    "        'Game': results['game'],\n",
    "        'Optimal Actions (%)': results['optimal_actions']\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='Environment', y='Optimal Actions (%)', data=df)\n",
    "    plt.title('Percentage of Optimal Actions')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_regret(results: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Plot regret.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'Environment': results['environment'],\n",
    "        'Game': results['game'],\n",
    "        'Regret': results['regret']\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='Environment', y='Regret', data=df)\n",
    "    plt.title('Distribution of Regret')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_action_history(results: Dict, game_idx: int) -> None:\n",
    "    \"\"\"\n",
    "    Plot action history for a specific game.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "        game_idx (int): Index of the game to plot\n",
    "    \"\"\"\n",
    "    env_name = results['environment'][game_idx]\n",
    "    actions = results['actions'][game_idx]\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(actions, 'o-', markersize=4)\n",
    "    plt.yticks([0, 1], ['Arm 1', 'Arm 2'])\n",
    "    plt.title(f'Action History for {env_name} - Game {results[\"game\"][game_idx]}')\n",
    "    plt.xlabel('Turn')\n",
    "    plt.ylabel('Action')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_reward_over_time(results: Dict, n_games: int = 5) -> None:\n",
    "    \"\"\"\n",
    "    Plot cumulative reward over time for selected games.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "        n_games (int): Number of games to plot\n",
    "    \"\"\"\n",
    "    # Select game indices to plot\n",
    "    env_names = list(set(results['environment']))\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    for env_name in env_names:\n",
    "        env_game_indices = [i for i, e in enumerate(results['environment']) if e == env_name]\n",
    "        \n",
    "        # Select a subset of games for this environment\n",
    "        selected_indices = env_game_indices[:n_games]\n",
    "        \n",
    "        for idx in selected_indices:\n",
    "            actions = results['actions'][idx]\n",
    "            turns = results['turns'][idx]\n",
    "            \n",
    "            # Reconstruct cumulative rewards\n",
    "            cum_rewards = np.cumsum([results['total_reward'][idx] / turns] * turns)\n",
    "            \n",
    "            plt.plot(cum_rewards, alpha=0.7, label=f\"{env_name} - Game {results['game'][idx]}\")\n",
    "    \n",
    "    plt.title('Cumulative Reward Over Time')\n",
    "    plt.xlabel('Turn')\n",
    "    plt.ylabel('Cumulative Reward')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def generate_statistical_summary(results: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a comprehensive statistical summary from experiment results.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Statistical summary\n",
    "    \"\"\"\n",
    "    # Convert results to DataFrame\n",
    "    data = []\n",
    "    for i in range(len(results['environment'])):\n",
    "        data.append({\n",
    "            'Environment': results['environment'][i],\n",
    "            'Game': results['game'][i],\n",
    "            'Total Reward': results['total_reward'][i],\n",
    "            'Average Reward': results['average_reward'][i],\n",
    "            'Turns': results['turns'][i],\n",
    "            'Optimal Actions (%)': results['optimal_actions'][i],\n",
    "            'Regret': results['regret'][i],\n",
    "            'Exploration Bonus': results['exploration_bonus'][i]\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Group by environment and calculate statistics\n",
    "    summary = df.groupby('Environment').agg({\n",
    "        'Total Reward': ['mean', 'std', 'min', 'max'],\n",
    "        'Average Reward': ['mean', 'std', 'min', 'max'],\n",
    "        'Optimal Actions (%)': ['mean', 'std', 'min', 'max'],\n",
    "        'Regret': ['mean', 'std', 'min', 'max'],\n",
    "        'Exploration Bonus': ['mean', 'std', 'min', 'max'],\n",
    "        'Turns': ['mean', 'count']\n",
    "    })\n",
    "    \n",
    "    # Rename the columns for better readability\n",
    "    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "def plot_performance_metrics(results: Dict, title_prefix: str = \"\") -> None:\n",
    "    \"\"\"\n",
    "    Plot comprehensive performance metrics for the agent.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "        title_prefix (str): Prefix for plot titles\n",
    "    \"\"\"\n",
    "    # Convert results to DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Environment': results['environment'],\n",
    "        'Game': results['game'],\n",
    "        'Average Reward': results['average_reward'],\n",
    "        'Optimal Actions (%)': results['optimal_actions'],\n",
    "        'Regret': results['regret'],\n",
    "        'Turns': results['turns'],\n",
    "        'Exploration Bonus': results['exploration_bonus']\n",
    "    })\n",
    "    \n",
    "    # Create a figure with multiple subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot average reward\n",
    "    sns.boxplot(x='Environment', y='Average Reward', data=df, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title(f'{title_prefix}Average Reward by Environment')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot optimal action percentage\n",
    "    sns.boxplot(x='Environment', y='Optimal Actions (%)', data=df, ax=axes[0, 1])\n",
    "    axes[0, 1].set_title(f'{title_prefix}Optimal Actions (%) by Environment')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot regret\n",
    "    sns.boxplot(x='Environment', y='Regret', data=df, ax=axes[1, 0])\n",
    "    axes[1, 0].set_title(f'{title_prefix}Regret by Environment')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot exploration bonus\n",
    "    sns.boxplot(x='Environment', y='Exploration Bonus', data=df, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title(f'{title_prefix}Exploration Bonus by Environment')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_simplified_results(results: Dict, agent_name: str = \"Agent\") -> None:\n",
    "    \"\"\"\n",
    "    Simplified visualization showing only aggregate results across all games.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "        agent_name (str): Name of the agent for titles\n",
    "    \"\"\"\n",
    "    # Convert results to DataFrame\n",
    "    data = []\n",
    "    for i in range(len(results['environment'])):\n",
    "        data.append({\n",
    "            'Environment': results['environment'][i],\n",
    "            'Game': results['game'][i],\n",
    "            'Average Reward': results['average_reward'][i],\n",
    "            'Optimal Actions (%)': results['optimal_actions'][i],\n",
    "            'Regret': results['regret'][i],\n",
    "            'Turns': results['turns'][i],\n",
    "            'Exploration Bonus': results['exploration_bonus'][i]\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Generate and display statistical summary\n",
    "    summary = df.groupby('Environment').agg({\n",
    "        'Average Reward': ['mean', 'std', 'min', 'max'],\n",
    "        'Optimal Actions (%)': ['mean', 'std', 'min', 'max'],\n",
    "        'Regret': ['mean', 'std'],\n",
    "        'Exploration Bonus': ['mean', 'std'],\n",
    "        'Turns': ['mean', 'std', 'min', 'max', 'count']\n",
    "    })\n",
    "    \n",
    "    # Rename the columns for better readability\n",
    "    summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "    \n",
    "    # Print agent name and summary statistics\n",
    "    print(f\"\\n==== {agent_name} Summary Statistics ====\")\n",
    "    summary_display = summary.round(3)  # Round to 3 decimal places for cleaner display\n",
    "    print(summary_display)\n",
    "    \n",
    "    # Create a figure with key metrics\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Average Reward\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.barplot(x='Environment', y='Average Reward', data=df, errorbar=('ci', 95), capsize=0.2)\n",
    "    plt.title(f'Average Reward by Environment')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Optimal Actions\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.barplot(x='Environment', y='Optimal Actions (%)', data=df, errorbar=('ci', 95), capsize=0.2)\n",
    "    plt.title(f'Optimal Actions (%) by Environment')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Regret\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.barplot(x='Environment', y='Regret', data=df, errorbar=('ci', 95), capsize=0.2)\n",
    "    plt.title(f'Regret by Environment')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Exploration Bonus\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.barplot(x='Environment', y='Exploration Bonus', data=df, errorbar=('ci', 95), capsize=0.2)\n",
    "    plt.title(f'Exploration Bonus by Environment')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"{agent_name} Performance\", fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_results(results: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Visualize the results of the experiment.\n",
    "    \n",
    "    Args:\n",
    "        results (Dict): Results from run_experiment\n",
    "    \"\"\"\n",
    "    # Convert results to pandas DataFrame for easier analysis\n",
    "    df = pd.DataFrame({\n",
    "        'Environment': results['environment'],\n",
    "        'Game': results['game'],\n",
    "        'Total Reward': results['total_reward'],\n",
    "        'Average Reward': results['average_reward'],\n",
    "        'Turns': results['turns'],\n",
    "        'Optimal Actions (%)': results['optimal_actions'],\n",
    "        'Regret': results['regret'],\n",
    "        'Exploration Bonus': results['exploration_bonus']\n",
    "    })\n",
    "    \n",
    "    # Summary statistics by environment\n",
    "    summary = df.groupby('Environment').agg({\n",
    "        'Total Reward': ['mean', 'std'],\n",
    "        'Average Reward': ['mean', 'std'],\n",
    "        'Optimal Actions (%)': ['mean', 'std'],\n",
    "        'Regret': ['mean', 'std'],\n",
    "        'Exploration Bonus': ['mean', 'std']\n",
    "    })\n",
    "    \n",
    "    print(\"===== Summary Statistics =====\")\n",
    "    print(summary)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Plot visualizations\n",
    "    plot_rewards_by_environment(results)\n",
    "    plot_optimal_actions(results)\n",
    "    plot_regret(results)\n",
    "    \n",
    "    # Plot action history for a game\n",
    "    if len(results['environment']) > 0:\n",
    "        plot_action_history(results, 0)\n",
    "    \n",
    "    # Plot reward over time for a subset of games\n",
    "    plot_reward_over_time(results, n_games=2)\n",
    "\n",
    "\n",
    "# ================ EXPLORATION BONUS ANALYSIS FUNCTIONS ================\n",
    "\n",
    "def visualize_bonus_comparison(results_by_bonus: Dict, title_prefix: str = \"\"):\n",
    "    \"\"\"\n",
    "    Visualize how performance metrics change with different exploration bonus values.\n",
    "    \n",
    "    Args:\n",
    "        results_by_bonus (Dict): Dictionary mapping bonus values to experiment results\n",
    "        title_prefix (str): Prefix for plot titles\n",
    "    \"\"\"\n",
    "    # Extract metrics for each bonus value\n",
    "    bonus_values = []\n",
    "    avg_rewards = []\n",
    "    optimal_actions = []\n",
    "    regrets = []\n",
    "    exploration_bonuses = []\n",
    "    \n",
    "    for bonus, results in sorted(results_by_bonus.items()):\n",
    "        bonus_values.append(bonus)\n",
    "        \n",
    "        # Compute mean metrics\n",
    "        df = pd.DataFrame({\n",
    "            'Average Reward': results['average_reward'],\n",
    "            'Optimal Actions (%)': results['optimal_actions'],\n",
    "            'Regret': results['regret'],\n",
    "            'Exploration Bonus': results['exploration_bonus']\n",
    "        })\n",
    "        \n",
    "        avg_rewards.append(df['Average Reward'].mean())\n",
    "        optimal_actions.append(df['Optimal Actions (%)'].mean())\n",
    "        regrets.append(df['Regret'].mean())\n",
    "        exploration_bonuses.append(df['Exploration Bonus'].mean())\n",
    "    \n",
    "    # Create figure with multiple plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    \n",
    "    # Plot average reward vs. exploration bonus\n",
    "    axes[0, 0].plot(bonus_values, avg_rewards, 'o-', linewidth=2, markersize=8, color='blue')\n",
    "    axes[0, 0].set_xlabel('Exploration Bonus Parameter (β)')\n",
    "    axes[0, 0].set_ylabel('Average Reward')\n",
    "    axes[0, 0].set_title(f'{title_prefix}: Average Reward vs. Exploration Bonus')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot optimal actions vs. exploration bonus\n",
    "    axes[0, 1].plot(bonus_values, optimal_actions, 'o-', linewidth=2, markersize=8, color='green')\n",
    "    axes[0, 1].set_xlabel('Exploration Bonus Parameter (β)')\n",
    "    axes[0, 1].set_ylabel('Optimal Actions (%)')\n",
    "    axes[0, 1].set_title(f'{title_prefix}: Optimal Actions vs. Exploration Bonus')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot regret vs. exploration bonus\n",
    "    axes[1, 0].plot(bonus_values, regrets, 'o-', linewidth=2, markersize=8, color='red')\n",
    "    axes[1, 0].set_xlabel('Exploration Bonus Parameter (β)')\n",
    "    axes[1, 0].set_ylabel('Regret')\n",
    "    axes[1, 0].set_title(f'{title_prefix}: Regret vs. Exploration Bonus')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot total exploration bonus received vs. parameter\n",
    "    axes[1, 1].plot(bonus_values, exploration_bonuses, 'o-', linewidth=2, markersize=8, color='purple')\n",
    "    axes[1, 1].set_xlabel('Exploration Bonus Parameter (β)')\n",
    "    axes[1, 1].set_ylabel('Total Exploration Bonus')\n",
    "    axes[1, 1].set_title(f'{title_prefix}: Accumulated Exploration Bonus')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"{title_prefix}: Impact of Exploration Bonus Parameter\", fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # Create boxplots for each metric\n",
    "    metrics = [\n",
    "        ('Average Reward', 'blue'), \n",
    "        ('Optimal Actions (%)', 'green'), \n",
    "        ('Regret', 'red')\n",
    "    ]\n",
    "    \n",
    "    for metric, color in metrics:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Prepare data for boxplot\n",
    "        data = []\n",
    "        labels = []\n",
    "        \n",
    "        for bonus, results in sorted(results_by_bonus.items()):\n",
    "            if metric == 'Average Reward':\n",
    "                data.append(results['average_reward'])\n",
    "            elif metric == 'Optimal Actions (%)':\n",
    "                data.append(results['optimal_actions'])\n",
    "            else:  # Regret\n",
    "                data.append(results['regret'])\n",
    "            \n",
    "            labels.append(f'β={bonus}')\n",
    "        \n",
    "        # Create boxplot\n",
    "        boxplot = plt.boxplot(data, labels=labels, patch_artist=True)\n",
    "        \n",
    "        # Color boxes\n",
    "        for box in boxplot['boxes']:\n",
    "            box.set(facecolor=color, alpha=0.6)\n",
    "        \n",
    "        plt.title(f'{title_prefix}: {metric} Distribution by Exploration Bonus')\n",
    "        plt.xlabel('Exploration Bonus Parameter (β)')\n",
    "        plt.ylabel(metric)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    # Print summary table\n",
    "    summary_data = []\n",
    "    for i, bonus in enumerate(bonus_values):\n",
    "        summary_data.append({\n",
    "            'Exploration Bonus (β)': bonus,\n",
    "            'Average Reward': avg_rewards[i],\n",
    "            'Optimal Actions (%)': optimal_actions[i],\n",
    "            'Regret': regrets[i],\n",
    "            'Total Exploration Bonus': exploration_bonuses[i]\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    print(f\"\\n{title_prefix}: Summary Table of Metrics by Exploration Bonus\")\n",
    "    print(summary_df.round(4))\n",
    "\n",
    "\n",
    "def visualize_all_problems_bonus_comparison(all_results: Dict):\n",
    "    \"\"\"\n",
    "    Visualize how exploration bonus affects performance across all problems.\n",
    "    \n",
    "    Args:\n",
    "        all_results (Dict): Nested dictionary: problem -> bonus -> results\n",
    "    \"\"\"\n",
    "    # Prepare data for plotting\n",
    "    problem_names = []\n",
    "    bonus_values = []\n",
    "    avg_rewards = []\n",
    "    optimal_actions = []\n",
    "    regrets = []\n",
    "    \n",
    "    for problem_name, problem_results in all_results.items():\n",
    "        for bonus, results in sorted(problem_results.items()):\n",
    "            problem_names.append(problem_name)\n",
    "            bonus_values.append(bonus)\n",
    "            \n",
    "            # Compute mean metrics\n",
    "            df = pd.DataFrame({\n",
    "                'Average Reward': results['average_reward'],\n",
    "                'Optimal Actions (%)': results['optimal_actions'],\n",
    "                'Regret': results['regret']\n",
    "            })\n",
    "            \n",
    "            avg_rewards.append(df['Average Reward'].mean())\n",
    "            optimal_actions.append(df['Optimal Actions (%)'].mean())\n",
    "            regrets.append(df['Regret'].mean())\n",
    "    \n",
    "    # Create DataFrame for plotting\n",
    "    plot_df = pd.DataFrame({\n",
    "        'Problem': problem_names,\n",
    "        'Exploration Bonus': bonus_values,\n",
    "        'Average Reward': avg_rewards,\n",
    "        'Optimal Actions (%)': optimal_actions,\n",
    "        'Regret': regrets\n",
    "    })\n",
    "    \n",
    "    # Create line plots showing each problem's performance across bonus values\n",
    "    metrics = ['Average Reward', 'Optimal Actions (%)', 'Regret']\n",
    "    colors = ['blue', 'green', 'red']\n",
    "    \n",
    "    for metric, color in zip(metrics, colors):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for problem in sorted(set(problem_names)):\n",
    "            problem_df = plot_df[plot_df['Problem'] == problem]\n",
    "            plt.plot(\n",
    "                problem_df['Exploration Bonus'], \n",
    "                problem_df[metric],\n",
    "                'o-',\n",
    "                linewidth=2,\n",
    "                markersize=8,\n",
    "                label=problem\n",
    "            )\n",
    "        \n",
    "        plt.xlabel('Exploration Bonus Parameter (β)')\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f'Effect of Exploration Bonus on {metric} Across Problems')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    # Create heatmaps showing the effect of bonus across problems\n",
    "    for metric in metrics:\n",
    "        # Reshape data for heatmap\n",
    "        pivot_df = plot_df.pivot_table(\n",
    "            index='Problem',\n",
    "            columns='Exploration Bonus',\n",
    "            values=metric\n",
    "        )\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(pivot_df, annot=True, cmap='YlGnBu', fmt='.3f')\n",
    "        plt.title(f'Heatmap of {metric} by Problem and Exploration Bonus')\n",
    "        plt.xlabel('Exploration Bonus Parameter (β)')\n",
    "        plt.ylabel('Problem')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def evaluate_problem_with_varying_bonus(problem_number: int, \n",
    "                                      bonus_values: List[float] = [0.01, 0.1, 0.5, 1.0],\n",
    "                                      fixed_turns: bool = True,\n",
    "                                      n_experiments: int = 50) -> Dict:\n",
    "    \"\"\"\n",
    "    Run experiments for a specific problem with varying exploration bonus values.\n",
    "    \n",
    "    Args:\n",
    "        problem_number (int): Problem number (1-4)\n",
    "        bonus_values (List[float]): List of exploration bonus values to test\n",
    "        fixed_turns (bool): Whether to use fixed T=100 or random T (1-300)\n",
    "        n_experiments (int): Number of experiments per bonus value\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for all bonus values\n",
    "    \"\"\"\n",
    "    agent_funcs = {\n",
    "        1: problem1_agent,\n",
    "        2: problem2_agent,\n",
    "        3: problem3_agent,\n",
    "        4: problem4_agent\n",
    "    }\n",
    "    \n",
    "    if problem_number not in agent_funcs:\n",
    "        raise ValueError(f\"Invalid problem number: {problem_number}. Must be 1-4.\")\n",
    "    \n",
    "    agent_func = agent_funcs[problem_number]\n",
    "    \n",
    "    # Store results for each bonus value\n",
    "    all_results = {}\n",
    "    \n",
    "    # Create a single progress bar for all bonus values\n",
    "    total_experiments = len(bonus_values) * n_experiments\n",
    "    with tqdm(total=total_experiments, desc=f\"Problem {problem_number} β Analysis\") as pbar:\n",
    "        for bonus in bonus_values:\n",
    "            # Create environment\n",
    "            env_class = [Problem1Environment, Problem2Environment, Problem3Environment, Problem4Environment][problem_number-1]\n",
    "            env_params = {\n",
    "                'exploration_bonus': bonus,\n",
    "                'state_count': 2\n",
    "            }\n",
    "            \n",
    "            # For Problem 4, add additional parameters\n",
    "            if problem_number == 4:\n",
    "                env_params.update({\n",
    "                    'known_transitions': True,\n",
    "                    'random_horizon': not fixed_turns,\n",
    "                    'joint_dynamics': False,\n",
    "                    'discount_factor': 0.95\n",
    "                })\n",
    "            \n",
    "            # Run without internal progress bar\n",
    "            results = run_experiment(\n",
    "                agent_func=agent_func,\n",
    "                env_class=env_class,\n",
    "                env_params=env_params,\n",
    "                n_games=n_experiments,\n",
    "                default_turns=100,\n",
    "                random_turns=not fixed_turns,\n",
    "                verbose=False,  # Disable internal progress bar\n",
    "                pbar=pbar      # Pass our progress bar\n",
    "            )\n",
    "            \n",
    "            all_results[bonus] = results\n",
    "    \n",
    "    # Visualize the comparative results\n",
    "    visualize_bonus_comparison(all_results, f\"Problem {problem_number}\")\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "def evaluate_problem1_with_varying_bonus(bonus_values: List[float] = [0.01, 0.1, 0.5, 1.0],\n",
    "                                        fixed_turns: bool = True,\n",
    "                                        n_experiments: int = 50) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate Problem 1 agent with varying exploration bonus values.\n",
    "    \n",
    "    Args:\n",
    "        bonus_values (List[float]): List of exploration bonus values to test\n",
    "        fixed_turns (bool): Whether to use fixed T=100 or random T (1-300)\n",
    "        n_experiments (int): Number of experiments per bonus value\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for all bonus values\n",
    "    \"\"\"\n",
    "    return evaluate_problem_with_varying_bonus(\n",
    "        problem_number=1,\n",
    "        bonus_values=bonus_values,\n",
    "        fixed_turns=fixed_turns,\n",
    "        n_experiments=n_experiments\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_problem2_with_varying_bonus(bonus_values: List[float] = [0.01, 0.1, 0.5, 1.0],\n",
    "                                        fixed_turns: bool = True,\n",
    "                                        n_experiments: int = 50) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate Problem 2 agent with varying exploration bonus values.\n",
    "    \n",
    "    Args:\n",
    "        bonus_values (List[float]): List of exploration bonus values to test\n",
    "        fixed_turns (bool): Whether to use fixed T=100 or random T (1-300)\n",
    "        n_experiments (int): Number of experiments per bonus value\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for all bonus values\n",
    "    \"\"\"\n",
    "    return evaluate_problem_with_varying_bonus(\n",
    "        problem_number=2,\n",
    "        bonus_values=bonus_values,\n",
    "        fixed_turns=fixed_turns,\n",
    "        n_experiments=n_experiments\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_problem3_with_varying_bonus(bonus_values: List[float] = [0.01, 0.1, 0.5, 1.0],\n",
    "                                        fixed_turns: bool = True,\n",
    "                                        n_experiments: int = 50) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate Problem 3 agent with varying exploration bonus values.\n",
    "    \n",
    "    Args:\n",
    "        bonus_values (List[float]): List of exploration bonus values to test\n",
    "        fixed_turns (bool): Whether to use fixed T=100 or random T (1-300)\n",
    "        n_experiments (int): Number of experiments per bonus value\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for all bonus values\n",
    "    \"\"\"\n",
    "    return evaluate_problem_with_varying_bonus(\n",
    "        problem_number=3,\n",
    "        bonus_values=bonus_values,\n",
    "        fixed_turns=fixed_turns,\n",
    "        n_experiments=n_experiments\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_problem4_with_varying_bonus(bonus_values: List[float] = [0.01, 0.1, 0.5, 1.0],\n",
    "                                        fixed_turns: bool = True,\n",
    "                                        n_experiments: int = 50) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate Problem 4 agent with varying exploration bonus values.\n",
    "    \n",
    "    Args:\n",
    "        bonus_values (List[float]): List of exploration bonus values to test\n",
    "        fixed_turns (bool): Whether to use fixed T=100 or random T (1-300)\n",
    "        n_experiments (int): Number of experiments per bonus value\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for all bonus values\n",
    "    \"\"\"\n",
    "    return evaluate_problem_with_varying_bonus(\n",
    "        problem_number=4,\n",
    "        bonus_values=bonus_values,\n",
    "        fixed_turns=fixed_turns,\n",
    "        n_experiments=n_experiments\n",
    "    )\n",
    "\n",
    "\n",
    "def run_all_agents_with_varying_bonus(bonus_values: List[float] = [0.01, 0.1, 0.5, 1.0],\n",
    "                                     fixed_turns: bool = True,\n",
    "                                     n_experiments: int = 50) -> Dict:\n",
    "    \"\"\"\n",
    "    Run experiments for all problems with varying exploration bonus values.\n",
    "    \n",
    "    Args:\n",
    "        bonus_values (List[float]): List of exploration bonus values to test\n",
    "        fixed_turns (bool): Whether to use fixed T=100 or random T (1-300)\n",
    "        n_experiments (int): Number of experiments per bonus value\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for all problems and bonus values\n",
    "    \"\"\"\n",
    "    # Store results for each problem\n",
    "    all_results = {}\n",
    "    \n",
    "    # Combined progress tracking for all problems\n",
    "    total_experiments = 4 * len(bonus_values) * n_experiments\n",
    "    with tqdm(total=total_experiments, desc=f\"Exploration Analysis\") as pbar:\n",
    "        for problem_number in range(1, 5):\n",
    "            # Get problem class\n",
    "            env_class = [Problem1Environment, Problem2Environment, Problem3Environment, Problem4Environment][problem_number-1]\n",
    "            agent_func = [problem1_agent, problem2_agent, problem3_agent, problem4_agent][problem_number-1]\n",
    "            \n",
    "            # Store results for each bonus value\n",
    "            problem_results = {}\n",
    "            \n",
    "            for bonus in bonus_values:\n",
    "                # Create environment\n",
    "                env_params = {\n",
    "                    'exploration_bonus': bonus,\n",
    "                    'state_count': 2\n",
    "                }\n",
    "                \n",
    "                # For Problem 4, add additional parameters\n",
    "                if problem_number == 4:\n",
    "                    env_params.update({\n",
    "                        'known_transitions': True,\n",
    "                        'random_horizon': not fixed_turns,\n",
    "                        'joint_dynamics': False,\n",
    "                        'discount_factor': 0.95\n",
    "                    })\n",
    "                \n",
    "                # Run without internal progress bar\n",
    "                results = run_experiment(\n",
    "                    agent_func=agent_func,\n",
    "                    env_class=env_class,\n",
    "                    env_params=env_params,\n",
    "                    n_games=n_experiments,\n",
    "                    default_turns=100,\n",
    "                    random_turns=not fixed_turns,\n",
    "                    verbose=False,  # Disable internal progress bar\n",
    "                    pbar=pbar      # Pass our progress bar\n",
    "                )\n",
    "                \n",
    "                problem_results[bonus] = results\n",
    "            \n",
    "            all_results[f\"problem{problem_number}\"] = problem_results\n",
    "            \n",
    "            # Visualize after each problem\n",
    "            print(f\"\\n==== Problem {problem_number} Exploration Analysis Results ====\")\n",
    "            visualize_bonus_comparison(problem_results, f\"Problem {problem_number}\")\n",
    "    \n",
    "    # Visualize comparative results across all problems\n",
    "    print(\"\\n==== Cross-Problem Exploration Analysis ====\")\n",
    "    visualize_all_problems_bonus_comparison(all_results)\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "def evaluate_problem1_agent(fixed_turns: bool = True, exploration_bonus: float = 0.1, n_experiments: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    Run standardized evaluation for Problem 1 agent.\n",
    "    \n",
    "    Args:\n",
    "        fixed_turns (bool): Whether to use fixed T=100 (True) or random T=1-300 (False)\n",
    "        exploration_bonus (float): Coefficient for exploration bonus (β)\n",
    "        n_experiments (int): Number of games/experiments to run\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for the Problem 1 agent\n",
    "    \"\"\"\n",
    "    # Run the experiment\n",
    "    results = run_standard_experiment(\n",
    "        agent_func=problem1_agent,\n",
    "        problem_number=1,\n",
    "        fixed_turns=fixed_turns,\n",
    "        exploration_bonus=exploration_bonus,\n",
    "        n_experiments=n_experiments\n",
    "    )\n",
    "    \n",
    "    # Visualize the results\n",
    "    turns_type = \"Fixed (T=100)\" if fixed_turns else \"Random (T=1-300)\"\n",
    "    agent_name = f\"Problem 1 Agent ({turns_type}, β={exploration_bonus}, n={n_experiments})\"\n",
    "    visualize_simplified_results(results, agent_name=agent_name)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_problem2_agent(fixed_turns: bool = True, exploration_bonus: float = 0.1, n_experiments: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    Run standardized evaluation for Problem 2 agent.\n",
    "    \n",
    "    Args:\n",
    "        fixed_turns (bool): Whether to use fixed T=100 (True) or random T=1-300 (False)\n",
    "        exploration_bonus (float): Coefficient for exploration bonus (β)\n",
    "        n_experiments (int): Number of games/experiments to run\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for the Problem 2 agent\n",
    "    \"\"\"\n",
    "    # Run the experiment\n",
    "    results = run_standard_experiment(\n",
    "        agent_func=problem2_agent,\n",
    "        problem_number=2,\n",
    "        fixed_turns=fixed_turns,\n",
    "        exploration_bonus=exploration_bonus,\n",
    "        n_experiments=n_experiments\n",
    "    )\n",
    "    \n",
    "    # Visualize the results\n",
    "    turns_type = \"Fixed (T=100)\" if fixed_turns else \"Random (T=1-300)\"\n",
    "    agent_name = f\"Problem 2 Agent ({turns_type}, β={exploration_bonus}, n={n_experiments})\"\n",
    "    visualize_simplified_results(results, agent_name=agent_name)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_problem3_agent(fixed_turns: bool = True, exploration_bonus: float = 0.1, n_experiments: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    Run standardized evaluation for Problem 3 agent.\n",
    "    \n",
    "    Args:\n",
    "        fixed_turns (bool): Whether to use fixed T=100 (True) or random T=1-300 (False)\n",
    "        exploration_bonus (float): Coefficient for exploration bonus (β)\n",
    "        n_experiments (int): Number of games/experiments to run\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for the Problem 3 agent\n",
    "    \"\"\"\n",
    "    # Run the experiment\n",
    "    results = run_standard_experiment(\n",
    "        agent_func=problem3_agent,\n",
    "        problem_number=3,\n",
    "        fixed_turns=fixed_turns,\n",
    "        exploration_bonus=exploration_bonus,\n",
    "        n_experiments=n_experiments\n",
    "    )\n",
    "    \n",
    "    # Visualize the results\n",
    "    turns_type = \"Fixed (T=100)\" if fixed_turns else \"Random (T=1-300)\"\n",
    "    agent_name = f\"Problem 3 Agent ({turns_type}, β={exploration_bonus}, n={n_experiments})\"\n",
    "    visualize_simplified_results(results, agent_name=agent_name)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def evaluate_problem4_agent(fixed_turns: bool = True, exploration_bonus: float = 0.1, n_experiments: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    Run standardized evaluation for Problem 4 agent.\n",
    "    \n",
    "    Args:\n",
    "        fixed_turns (bool): Whether to use fixed T=100 (True) or random T=1-300 (False)\n",
    "        exploration_bonus (float): Coefficient for exploration bonus (β)\n",
    "        n_experiments (int): Number of games/experiments to run\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Results for the Problem 4 agent\n",
    "    \"\"\"\n",
    "    # Run the experiment\n",
    "    results = run_standard_experiment(\n",
    "        agent_func=problem4_agent,\n",
    "        problem_number=4,\n",
    "        fixed_turns=fixed_turns,\n",
    "        exploration_bonus=exploration_bonus,\n",
    "        n_experiments=n_experiments\n",
    "    )\n",
    "    \n",
    "    # Visualize the results\n",
    "    turns_type = \"Fixed (T=100)\" if fixed_turns else \"Random (T=1-300)\"\n",
    "    agent_name = f\"Problem 4 Agent ({turns_type}, β={exploration_bonus}, n={n_experiments})\"\n",
    "    visualize_simplified_results(results, agent_name=agent_name)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def run_all_agents(fixed_turns: bool = True, exploration_bonus: float = 0.1, n_experiments: int = 100) -> Dict:\n",
    "    \"\"\"\n",
    "    Run standard evaluations for all agent types.\n",
    "    \n",
    "    Args:\n",
    "        fixed_turns (bool): Whether to use fixed T=100 (True) or random T=1-300 (False)\n",
    "        exploration_bonus (float): Coefficient for exploration bonus (β)\n",
    "        n_experiments (int): Number of games/experiments to run for each agent\n",
    "    \n",
    "    Returns:\n",
    "        Dict: Comprehensive results for all agents\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Evaluate each agent type\n",
    "    print(\"\\n==== Running Problem 1 Agent ====\")\n",
    "    results[\"problem1\"] = evaluate_problem1_agent(fixed_turns=fixed_turns, \n",
    "                                                 exploration_bonus=exploration_bonus, \n",
    "                                                 n_experiments=n_experiments)\n",
    "    \n",
    "    print(\"\\n==== Running Problem 2 Agent ====\")\n",
    "    results[\"problem2\"] = evaluate_problem2_agent(fixed_turns=fixed_turns, \n",
    "                                                 exploration_bonus=exploration_bonus, \n",
    "                                                 n_experiments=n_experiments)\n",
    "    \n",
    "    print(\"\\n==== Running Problem 3 Agent ====\")\n",
    "    results[\"problem3\"] = evaluate_problem3_agent(fixed_turns=fixed_turns, \n",
    "                                                 exploration_bonus=exploration_bonus, \n",
    "                                                 n_experiments=n_experiments)\n",
    "    \n",
    "    print(\"\\n==== Running Problem 4 Agent ====\")\n",
    "    results[\"problem4\"] = evaluate_problem4_agent(fixed_turns=fixed_turns, \n",
    "                                                 exploration_bonus=exploration_bonus, \n",
    "                                                 n_experiments=n_experiments)\n",
    "    \n",
    "    # Compare agents\n",
    "    print(\"\\n==== Agent Comparison ====\")\n",
    "    compare_all_agents(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def compare_all_agents(all_results: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Compare the performance of all agents.\n",
    "    \n",
    "    Args:\n",
    "        all_results (Dict): Results from run_all_agents\n",
    "    \"\"\"\n",
    "    # Prepare data for comparison\n",
    "    comparison_data = []\n",
    "    \n",
    "    # Collect results from each agent\n",
    "    for agent_name, results in all_results.items():\n",
    "        # Calculate mean performance metrics for this agent\n",
    "        df = pd.DataFrame({\n",
    "            'Environment': results['environment'],\n",
    "            'Average Reward': results['average_reward'],\n",
    "            'Optimal Actions (%)': results['optimal_actions'],\n",
    "            'Regret': results['regret'],\n",
    "            'Exploration Bonus': results['exploration_bonus']\n",
    "        })\n",
    "        \n",
    "        # Get mean values\n",
    "        mean_reward = df['Average Reward'].mean()\n",
    "        mean_optimal = df['Optimal Actions (%)'].mean()\n",
    "        mean_regret = df['Regret'].mean()\n",
    "        mean_bonus = df['Exploration Bonus'].mean()\n",
    "        \n",
    "        # Add to comparison data\n",
    "        comparison_data.append({\n",
    "            'Agent': agent_name,\n",
    "            'Average Reward': mean_reward,\n",
    "            'Optimal Actions (%)': mean_optimal,\n",
    "            'Regret': mean_regret,\n",
    "            'Exploration Bonus': mean_bonus\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Display comprehensive summary table (rounded to 3 decimal places for readability)\n",
    "    print(\"\\n===== AGENT COMPARISON SUMMARY =====\")\n",
    "    print(\"\\nPerformance Metrics by Agent:\")\n",
    "    display_df = comparison_df.round(3)\n",
    "    print(display_df)\n",
    "    \n",
    "    # Create comparison plots\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Average Reward\n",
    "    plt.subplot(2, 2, 1)\n",
    "    sns.barplot(x='Agent', y='Average Reward', data=comparison_df)\n",
    "    plt.title('Average Reward by Agent')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Optimal Actions\n",
    "    plt.subplot(2, 2, 2)\n",
    "    sns.barplot(x='Agent', y='Optimal Actions (%)', data=comparison_df)\n",
    "    plt.title('Optimal Actions (%) by Agent')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Regret\n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.barplot(x='Agent', y='Regret', data=comparison_df)\n",
    "    plt.title('Regret by Agent')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Exploration Bonus\n",
    "    plt.subplot(2, 2, 4)\n",
    "    sns.barplot(x='Agent', y='Exploration Bonus', data=comparison_df)\n",
    "    plt.title('Exploration Bonus by Agent')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"Agent Performance Comparison\", fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate summary of best agent\n",
    "    print(\"\\nBest Agent (Based on Average Reward):\")\n",
    "    best_agent = comparison_df.loc[comparison_df['Average Reward'].idxmax()]\n",
    "    print(best_agent[['Agent', 'Average Reward']].to_string())\n",
    "\n",
    "\n",
    "# ================ AGENT FUNCTIONS ================\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_regularization_lambdas(n, low=1e-5, high=1e2, seed=None):\n",
    "    \"\"\"\n",
    "    Generate n lambda values from a log-uniform distribution.\n",
    "\n",
    "    Parameters:\n",
    "    - n (int): Number of lambda values to generate.\n",
    "    - low (float): Lower bound of the range (must be > 0).\n",
    "    - high (float): Upper bound of the range.\n",
    "    - seed (int or None): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - lambdas (np.ndarray): Array of shape (n,) with sampled lambda values.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    log_low = np.log10(low)\n",
    "    log_high = np.log10(high)\n",
    "    lambdas = np.power(10, np.random.uniform(log_low, log_high, n))\n",
    "    return lambdas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents & Core Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem1_agent(env_info: Dict) -> int:\n",
    "    \"\"\"\n",
    "    Agent for Problem 1: Independent, Known Transitions, Fixed T, Hidden State + Exploration Bonus\n",
    "    \n",
    "    This agent implements a simple belief state update using the known transition matrices\n",
    "    and combines exploitation with exploration via the exploration bonus.\n",
    "    \n",
    "    Args:\n",
    "        env_info (Dict): Dictionary containing:\n",
    "            - current_turn (int): Current turn number (starts at 0)\n",
    "            - total_turns (int): Total number of turns in the game (fixed T)\n",
    "            - transition_matrices (List[np.ndarray]): Known transition matrices for both arms.\n",
    "                                                     Each is a 2D array of shape (state_count, state_count)\n",
    "                                                     where transition_matrices[i][s1, s2] is the probability\n",
    "                                                     of transitioning from state s1 to s2 for arm i.\n",
    "            - state_rewards (List[float]): Rewards associated with each state (e.g., [0.2, 0.8] for\n",
    "                                         \"bad\" and \"good\" states)\n",
    "            - history (Dict): Dictionary with past data:\n",
    "                - 'actions' (List[int]): Previous actions taken (0 for arm 1, 1 for arm 2)\n",
    "                - 'rewards' (List[float]): Rewards received for each action\n",
    "                - 'exploration_bonus' (List[float]): Exploration bonuses received\n",
    "    \n",
    "    Returns:\n",
    "        int: The action to take (0 for arm 1, 1 for arm 2)\n",
    "    \"\"\"\n",
    "    # TODO: Students implement this agent\n",
    "    \n",
    "    # Example implementation (random choice)\n",
    "    return np.random.randint(0, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnos fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = evaluate_problem1_agent(fixed_turns=True, exploration_bonus=.1, n_experiments=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnos no fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = evaluate_problem1_agent(fixed_turns=False, exploration_bonus=.1, n_experiments=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid para variar bonus de exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_values = sample_regularization_lambdas(n=100,low=1e-5,high=5)\n",
    "a = evaluate_problem1_with_varying_bonus(bonus_values=bonus_values, fixed_turns=False, n_experiments=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def problem2_agent(env_info: Dict) -> int:\n",
    "    \"\"\"\n",
    "    Agent for Problem 2: Independent, Unknown Transitions, Fixed T, Hidden State + Exploration Bonus\n",
    "    \n",
    "    This agent must learn the transition matrices through experience while also\n",
    "    balancing exploration and exploitation. Since the transition matrices are unknown,\n",
    "    the agent needs to maintain estimates of them based on observed rewards.\n",
    "    \n",
    "    Args:\n",
    "        env_info (Dict): Dictionary containing:\n",
    "            - current_turn (int): Current turn number (starts at 0)\n",
    "            - total_turns (int): Total number of turns in the game (fixed T)\n",
    "            - history (Dict): Dictionary with past data:\n",
    "                - 'actions' (List[int]): Previous actions taken (0 for arm 1, 1 for arm 2)\n",
    "                - 'rewards' (List[float]): Rewards received for each action (typically 0.0 or 1.0 \n",
    "                                         for Bernoulli rewards)\n",
    "                - 'exploration_bonus' (List[float]): Exploration bonuses received\n",
    "    \n",
    "    Note: \n",
    "        Unlike Problem 1, no transition matrices are provided because they are unknown.\n",
    "        The agent must infer the hidden states and transition dynamics from the reward sequence.\n",
    "    \n",
    "    Returns:\n",
    "        int: The action to take (0 for arm 1, 1 for arm 2)\n",
    "    \"\"\"\n",
    "    # TODO: Students implement this agent\n",
    "    \n",
    "    # Example implementation (random choice)\n",
    "    return np.random.randint(0, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnos Fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = evaluate_problem2_agent(fixed_turns=True, exploration_bonus=.1, n_experiments=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnos no Fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = evaluate_problem2_agent(fixed_turns=False, exploration_bonus=.1, n_experiments=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid para variar bonus de exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_values = sample_regularization_lambdas(n=100,low=1e-5,high=5)\n",
    "a = evaluate_problem2_with_varying_bonus(bonus_values=bonus_values, fixed_turns=False, n_experiments=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def problem3_agent(env_info: Dict) -> int:\n",
    "    \"\"\"\n",
    "    Agent for Problem 3: Dependent (Joint), Known Transitions, Fixed T, Hidden State + Exploration Bonus\n",
    "    \n",
    "    This agent must track the joint belief state across both arms using the known\n",
    "    joint transition matrix. The key difference from Problem 1 is that the states of\n",
    "    the two arms can be correlated, so pulling one arm may provide information about\n",
    "    the state of the other arm.\n",
    "    \n",
    "    Args:\n",
    "        env_info (Dict): Dictionary containing:\n",
    "            - current_turn (int): Current turn number (starts at 0)\n",
    "            - total_turns (int): Total number of turns in the game (fixed T)\n",
    "            - joint_transition_matrix (np.ndarray): Known joint transition matrix of shape \n",
    "                                                 (joint_state_count, joint_state_count).\n",
    "                                                 For state_count=2, this will be a 4x4 matrix where\n",
    "                                                 the indices represent joint states: \n",
    "                                                 0=(0,0), 1=(0,1), 2=(1,0), 3=(1,1)\n",
    "            - state_count (int): Number of states per arm (e.g., 2 for \"good\"/\"bad\")\n",
    "            - state_rewards (List[float]): Rewards associated with each state (e.g., [0.2, 0.8])\n",
    "            - history (Dict): Dictionary with past data:\n",
    "                - 'actions' (List[int]): Previous actions taken (0 for arm 1, 1 for arm 2)\n",
    "                - 'rewards' (List[float]): Rewards received for each action\n",
    "                - 'exploration_bonus' (List[float]): Exploration bonuses received\n",
    "    \n",
    "    Returns:\n",
    "        int: The action to take (0 for arm 1, 1 for arm 2)\n",
    "    \"\"\"\n",
    "    # TODO: Students implement this agent\n",
    "    \n",
    "    # Example implementation (random choice)\n",
    "    return np.random.randint(0, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnos Fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = evaluate_problem3_agent(fixed_turns=True, exploration_bonus=.1, n_experiments=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnos no fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = evaluate_problem3_agent(fixed_turns=False, exploration_bonus=.1, n_experiments=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid para variar bonus de exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_values = sample_regularization_lambdas(n=100,low=1e-5,high=5)\n",
    "a = evaluate_problem3_with_varying_bonus(bonus_values=bonus_values, fixed_turns=False, n_experiments=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def problem4_agent(env_info: Dict) -> int:\n",
    "    \"\"\"\n",
    "    Agent for Problem 4: Partially Observable / Possibly Unknown / Possibly Random T\n",
    "    \n",
    "    This is the most general agent that must handle various scenarios:\n",
    "    - Transitions may be known or unknown\n",
    "    - Horizon may be fixed or random\n",
    "    - Arms may have independent or joint dynamics\n",
    "    \n",
    "    Args:\n",
    "        env_info (Dict): Dictionary containing:\n",
    "            - current_turn (int): Current turn number (starts at 0)\n",
    "            - total_turns (int, optional): Total number of turns. May be missing if random_horizon is True.\n",
    "            \n",
    "            # Transition information (may be present or absent):\n",
    "            - transition_matrices (List[np.ndarray], optional): If present, contains the known independent \n",
    "              transition matrices for both arms. Each matrix has shape (state_count, state_count).\n",
    "            - joint_transition_matrix (np.ndarray, optional): If present, contains the known joint \n",
    "              transition matrix of shape (joint_state_count, joint_state_count).\n",
    "            - joint_dynamics (bool, optional): If True, arms have joint dynamics; if False, they're independent.\n",
    "              Only present if transitions are known.\n",
    "            - state_rewards (List[float], optional): Rewards associated with each state. Only present \n",
    "              if transitions are known.\n",
    "            \n",
    "            # Horizon information:\n",
    "            - random_horizon (bool, optional): If True, the horizon is random and not fixed.\n",
    "            - discount_factor (float, optional): Discount factor for infinite/random horizon. Only \n",
    "              present if random_horizon is True.\n",
    "            \n",
    "            # History:\n",
    "            - history (Dict): Dictionary with past data:\n",
    "                - 'actions' (List[int]): Previous actions taken (0 for arm 1, 1 for arm 2)\n",
    "                - 'rewards' (List[float]): Rewards received for each action\n",
    "                - 'exploration_bonus' (List[float]): Exploration bonuses received\n",
    "    \n",
    "    Returns:\n",
    "        int: The action to take (0 for arm 1, 1 for arm 2)\n",
    "    \n",
    "    Note:\n",
    "        This is the most challenging scenario as it combines all variations:\n",
    "        - May need to learn transition dynamics if unknown\n",
    "        - May need to handle joint state tracking\n",
    "        - May need to use discounting for random/infinite horizon\n",
    "        - Always needs to balance exploration vs. exploitation\n",
    "    \"\"\"\n",
    "    # TODO: Students implement this agent\n",
    "    \n",
    "    # Example implementation (random choice)\n",
    "    return np.random.randint(0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnos Fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = evaluate_problem4_agent(fixed_turns=True, exploration_bonus=.1, n_experiments=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turnos no fijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = evaluate_problem4_agent(fixed_turns=False, exploration_bonus=.1, n_experiments=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid para variar bonus de exploración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus_values = sample_regularization_lambdas(n=100,low=1e-5,high=5)\n",
    "a = evaluate_problem4_with_varying_bonus(bonus_values=bonus_values, fixed_turns=False, n_experiments=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_results = run_all_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_all_agents_with_varying_bonus()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
